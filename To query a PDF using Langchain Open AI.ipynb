{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bdde3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.1.12-py3-none-any.whl (809 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.31\n",
      "  Downloading langchain_core-0.1.32-py3-none-any.whl (260 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from langchain) (3.8.3)\n",
      "Collecting langsmith<0.2.0,>=0.1.17\n",
      "  Downloading langsmith-0.1.29-py3-none-any.whl (70 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from langchain) (1.20.1)\n",
      "Collecting langchain-community<0.1,>=0.0.28\n",
      "  Downloading langchain_community-0.0.28-py3-none-any.whl (1.8 MB)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from langchain) (8.1.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from langchain) (5.4.1)\n",
      "Collecting pydantic<3,>=1\n",
      "  Downloading pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from langchain) (1.4.7)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from langchain) (2.25.1)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (20.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.1.1)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting anyio<5,>=3\n",
      "  Downloading anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "Collecting packaging<24.0,>=23.2\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (2.10)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (4.4.0)\n",
      "Collecting exceptiongroup>=1.0.2\n",
      "  Downloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.2.0)\n",
      "Collecting orjson<4.0.0,>=3.9.14\n",
      "  Downloading orjson-3.9.15-cp38-none-win_amd64.whl (135 kB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting pydantic-core==2.16.3\n",
      "  Downloading pydantic_core-2.16.3-cp38-none-win_amd64.whl (1.9 MB)\n",
      "Collecting typing-extensions>=4.1\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (1.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n",
      "Installing collected packages: typing-extensions, pydantic-core, annotated-types, pydantic, packaging, orjson, jsonpointer, exceptiongroup, typing-inspect, marshmallow, langsmith, jsonpatch, anyio, langchain-core, dataclasses-json, langchain-text-splitters, langchain-community, langchain\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.4.0\n",
      "    Uninstalling typing-extensions-4.4.0:\n",
      "      Successfully uninstalled typing-extensions-4.4.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.9\n",
      "    Uninstalling packaging-20.9:\n",
      "      Successfully uninstalled packaging-20.9\n",
      "  Attempting uninstall: orjson\n",
      "    Found existing installation: orjson 3.8.6\n",
      "    Uninstalling orjson-3.8.6:\n",
      "      Successfully uninstalled orjson-3.8.6\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 2.2.0\n",
      "    Uninstalling anyio-2.2.0:\n",
      "      Successfully uninstalled anyio-2.2.0\n",
      "Successfully installed annotated-types-0.6.0 anyio-4.3.0 dataclasses-json-0.6.4 exceptiongroup-1.2.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.12 langchain-community-0.0.28 langchain-core-0.1.32 langchain-text-splitters-0.0.1 langsmith-0.1.29 marshmallow-3.21.1 orjson-3.9.15 packaging-23.2 pydantic-2.6.4 pydantic-core-2.16.3 typing-extensions-4.10.0 typing-inspect-0.9.0\n",
      "Collecting openai\n",
      "  Downloading openai-1.14.2-py3-none-any.whl (262 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from openai) (4.59.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from openai) (4.3.0)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from openai) (2.6.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (2.10)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2020.12.5)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Installing collected packages: h11, httpcore, httpx, distro, openai\n",
      "Successfully installed distro-1.9.0 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.2\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from PyPDF2) (4.10.0)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0-cp38-cp38-win_amd64.whl (14.5 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.20.1)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0\n",
      "Collecting tiktoken"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.9.1 requires ruamel-yaml, which is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading tiktoken-0.6.0-cp38-cp38-win_amd64.whl (798 kB)\n",
      "Collecting requests>=2.26.0\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2023.12.25-cp38-cp38-win_amd64.whl (269 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2020.12.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kpuni\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.10)\n",
      "Installing collected packages: requests, regex, tiktoken\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.25.1\n",
      "    Uninstalling requests-2.25.1:\n",
      "      Successfully uninstalled requests-2.25.1\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2021.4.4\n",
      "    Uninstalling regex-2021.4.4:\n",
      "      Successfully uninstalled regex-2021.4.4\n",
      "Successfully installed regex-2023.12.25 requests-2.31.0 tiktoken-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install openai\n",
    "!pip install PyPDF2\n",
    "!pip install faiss-cpu // to read from pdf\n",
    "!pip install tiktoken  // to create tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c32cff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings //OpenAI embeddings -\n",
    "from langchain.text_splitter import CharacterTextSplitter // split content of text\n",
    "from langchain.vectorstores import FAISS //text stored in vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7332f88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1719835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfreader = PdfReader('KeerthanaPunithan(KP)_ DataScientist.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d75ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Concatenate\n",
    "# read text from pdf\n",
    "raw_text = ''\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68efe696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"KEERTHANA PUNITHAN  \\nData Scientist   \\n \\n               https://www.linkedin.com/in/keerthana -punithan -2ab51a28/                                      +1(747) 609-7059                                               \\n             https://github.com/Keerthana2701                                                                                        keerthanapunithan@gmail.com  \\n               https://artificialintelligencemastery.blogspot.com                                                             Los Angeles, CA        \\n                                                                                                                                                                   \\n \\n \\n \\n \\n \\n   \\n \\n \\n  \\n \\n \\n \\nSummary  \\n                                                                                                                                 \\n\\uf0b7 7 years and 3 months  of professional  experience as Data Scientist ( Machine  Learning / Deep \\nLearning / Computer Vision / Natural Language Processing)  and Business Intelligence \\nReporting  (MicroStrategy  and Tableau BI Report Develop ment )\\n \\n\\uf0b7 Demonstrated ability in Banking and Health Insurance Sectors implementing  Data Science,  \\nStatistics and Model  Building with Machine Learning algorithms, CNN and RNN models,  \\nNatural  Language Processing,  Computer Vision algorithms,  SQL, BI analytical reports to \\nidentify KPIs and strategic planning as per end user requirement  \\n\\uf0b7 Sound exposure and experience in Data Analysis , Data Modelling, Dashboards, DevOps , Data \\nVisualization, Statistical modelling,  Data S cience Pipelines encompassing Feature \\nEngineering , Model Validati on,  Deployment and MLOPs  \\n \\nEducation  \\n \\n• Post Graduate Program in Artificial Intelligence and Machine Learning , University \\nof Texas , Austin - McCombs School of Business , 2019  – 2021 , June ( Graduated)  \\n• Master of Business Administration (Human Resource),  Anna University, Chennai, \\nIndia with an aggregate of 73% during 2012 -2014  (Graduated)  \\n• Bachelor of Engineering (Computer Science and Engineering ), Sri Siva Subramanian  \\nNadar Engineering College (SSN), Chennai with an aggregate of 83.05% during 2007 -\\n2011  (Graduated)  \\n• Higher Secondary from Velammal Matric Higher Sec School, Chennai  with an aggregate \\nof 96.08%, during 2005 -2007. School topper in High School  \\n \\n \\n Post Graduate Program in Artificial Intelligence and Machine Learning  \\n                                                                                                                                              (Aug 2019 –June 2021)       \\n \\nInstitution:  The University of Texas at Austin McCombs School of B usiness , TX \\nhttps://vrfy.digital/zPpgp  \\n \\nTools, Technology,Libraries,IDEs:   Python, R, NLP, TensorFlow, Keras, Spacy, Heroku, Advanced \\nStatistics, Google Cloud Platform, OpenCV, Gensim, PySpark, Docker, NLTK, ANN,CNN, RNN , Dlib \\n,Scikitlearn, Numpy, Pandas , Spyder, Jupyter Notebook, PyCharm,Matplotlib, Seaborn, MLOPs, \\nMLflow,  Kubeflow, DVC,  Flask, Djano,MongoDB, DevOps, Azure Cloud, Azure ML Studio,  AWS \\nSageMaker, AWS EC2 , AWS Lambda,  Hive, Kafka.  \\nProjects:  \\n\\uf0b7 Applied Statistics project that uses Hypothesis Testing, Statistical inference and \\nVisualization to leverage customer's heal th information for checking statistical evidence \\nto make valuable decisions of insurance business like charges for health insurance.  \\n\\uf0b7 Supervised Learning project that identifies potential loan customers for Bank using \\nclassification techniques. Compared mod els built with Logistic Regression and KNN \\nalgorithm in order to select the best performing one.  \\n\\uf0b7 Ensemble Techniques project involved using classification algorithms and Ensemble \\ntechniques to diagnose Parkinson’s Disease (PD) using the patient voice recor ding data. \\nVarious models were used including Naive Bayes, Logistic Regression, SVM, Decision \\nTree, Random Forest etc. and comparison of accuracy across these models was done to \\nfinalize the model for prediction.  \\n\\uf0b7 Unsupervised Learning project that classifi ed vehicles into different types based on \\nsilhouettes which may be viewed from many angles. Used PCA in order to reduce \\ndimensionality and SVM for classification.  \\n\\uf0b7 Featurization and Model Tuning Project that analyses the modelling of strength of high -\\nperfor mance  concrete using ML and feature engineering techniques and hyper \\nparameter tuning and built ML model using different regression algorithms like XGBoost, \\nAda Boost, Random Forest Regressor etc.  \\n\\uf0b7 Building recommendation systems for Amazon products based o n popularity -based \\nmodel and a collaborative Filtering model  \\n\\uf0b7 Implementing an Image classification neural network to classify Street House View \\nNumbers  \\n\\uf0b7 Computer Vision Project based on CNN, Transfer Learning and Tensor flow to deliver  a \\nGUI powered by compu ter vision techniques to solve the problem of a botanical research \\ngroup  \\n\\uf0b7 Advanced Computer Vision Project for object detection model for highlighting human \\nfaces to automate the process of providing information of cast and crew while streaming \\nand implemen ted face recognition model including imputing dynamic bounding boxes.  \\uf0b7 Statistical Natural Language Processing Project based on NLP and supervised  learning \\nclassifier techniques to implement NLP classifier for a digital marketing company that \\ncan use input text parameters to determine the category based on the blog’s literature \\nfrom the corpus of numerous blogs curated and maintained.  \\n\\uf0b7 Sequential Natural Language Processing Project based on NLP and sequential NLP based \\ntext classifier for a social media analy tics company using input text parameters to \\ndetermine the customer sentiments for sarcasm detection using LST M \\n \\nINTERNAL  PROJECTS  \\n \\n\\uf0b7 Developed end to end machine learning prediction model using random forest model and \\nhyper parameter techniques and deployed using Flask API in Heroku (PAAS) and Google \\nCloud Platform.  \\n\\uf0b7 Implemented Neural Network prediction model for the bank customer exit and diabetes \\nprediction using keras, tensor flow and Pytorch  and deployed the model in AWS EC2.  \\n\\uf0b7 Worked on PySpark to develop and execute machine learning applications like building \\nregression and random forest models in AWS SageMaker with deployment . \\n\\uf0b7 Designed and built machine leaning model using XGBoost ensemble technique and deployed \\nit in Azure Cloud.  \\n\\uf0b7 Built predictive analysi s model using decision tree regressor using Azure Machine Learning \\nStudio.  \\n\\uf0b7 Designed convolutional model for image processing using Keras using various data \\naugmentation and optimizers  and built function in AWS Lambda with the CNN code and \\ndeployed it.  \\n\\uf0b7 Built an object detection model for car detection using YOLO algorithm and dealing with \\nbounding boxes using CNN  and deployed in Amazon EC2 with flask API to create desktop \\napplication and data storage in S3 bucket  \\n\\uf0b7 Implemented transfer learning using pretrai ned VGG16 and ResNet for face recognition and \\nfine tuning  and deployed with Dockers and Flassger  \\n\\uf0b7 Built, trained and deployed machine learning models in AWS SageMaker  \\n\\uf0b7 Built Computer Vision project on identifying Shape by contours using OpenCV.  \\n\\uf0b7 Implemented C omputer Vision project on counting circles and ellipses using blob detection \\nand blob filtering technique that includes area, circularity using OpenCV.  \\n\\uf0b7 Object detection using template matching and image feature detection using SIFT, SURF, \\nFAST, BRIEF and O RBs using OpenCV  \\n\\uf0b7 Worked on pertained HAAR Cascade classifiers to detect face and eye, merging faces and \\nfacial landmarks extraction using dlib’s pretrained face detector.  \\n\\uf0b7 Automating dynamic bounding box imputation to locate car on road on the live video fi le \\nusing transfer learning CNN technique  \\n\\uf0b7 Motion Analysis and Object tracking that includes algorithms like filtering by color, \\nbackground subtraction and Meanshift in OpenCV  \\n\\uf0b7 Build a pneumonia detection system, to locate the position of inflammation in an i mage and \\npredict if patient test positive by building model using Unet and Resnet architecture  \\uf0b7 NLP Text classification and text summarization Project that uses Logistic Regression and \\nNLP techniques to gets the main points of document based on sentence sc ore and uses \\nBeautiful Soup to extract data from the web page HTML  \\n\\uf0b7 Automatic Ticket Assignment using Machine learning, RNN and LSTM Model and using NLP \\nword embedding techniques like word2vec, doc2vec, pretrained glove, TFIDF vectorizer \\nwith topic modelli ng techniques like LSA and LDA.  \\n \\n \\nCAPSTONE PROJECT  \\n \\nCHATBOT INTERFACE -TEXT CLASSIFICATION USING NATURAL LANGUAGE PROCESSING  \\n \\nThis Project aims at designing a Machine Learning, Deep Learning and Natural Language Processing \\nbased chatbot utility which can help the professionals to highlight the safety risk as per the incident \\ndescription. The Chatbot helps technicians by automatin g the interaction process to get access for \\nimmediate help and support, avoiding the need for manual interaction.The model was designed, \\ntrained and tested for Machine learning classifiers, Neural Network Classifier and LSTM Classifier.   \\n \\nAccuracy, Recall , Precision score , F1 score, confusion matrix were computed for all the models and \\nthe best performing model was selected for building User Interface Chatbot using Flask web \\napplication wi th JavaScript, CSS and deployed in AWS EC2 .This chatbot will help us ers analyze the \\nrisk level of the accidents.The leading cause of accidents were explored through a combination of \\ndeep learning models and NLP techniques.  \\n \\n \\nWork Experience  \\n                                                                                          \\nData Scientist                                                                                                                 (Sep 2017 – Aug 2018)  \\n \\nClient : Standard Chartered Bank , Hong Kong  \\nEmployer : Infocepts, Chennai  \\n \\nTools and Technology:  NLP , NLU , Python, Machine Learning, Deep Lear ning , NLTK lib, GCP, \\nAzure, Gensim,Spacy , MicroStrategy 9.4.x, Tableau 9.x, JIRA, SQL Server, Hadoop, Hive, Dev Ops \\nProject : Confidential  \\nRoles and Responsibilities:  \\n \\n• Involved in Requirements Gathering and Estimations  and data modelling in \\ncoordination with Data engineering team for maintaining the bank database  to source  \\ndata  • Built models using machine learning and statistical techniques to develop and build \\nalgorithms to make predictions for retail banking for investment decision making  \\n• Worked extensively on building CNN models for credit card number image processing \\nusing OpenCV and transfer learning  techniques  \\n• Build models using word embedding techniques like BOW, TF -IDF, pretrained Glove and \\nWord2Vec  and developed NLP models for text classification  and t opic extraction  and \\ncreated pipelines for development and testing  for investment decision making  \\n• Designed and built robust RNN and LSTM Models using TensorFlow and Keras and \\nevaluated the model accuracy for secure transactions  by analyzing data points to \\nprevent fraud transactions  \\n• Financial monitoring and risk management by associating a risk score to help bank \\nmake decision about customer preferences  \\n• Deployment of model in cloud platform  for the end users to access the web API  and \\nprovided model monitoring  support  \\n• Collaborated with data engineers and operation team to perform data extraction and \\ndata analysis  \\n• Created Go -Live dashboards using Microstrategy   which includes implementation plan, \\nUser Access, User Approval, Application Instruction Guide and SLA documents and \\nprovided user support  \\n• Worked with the ETL Team for loading tables based on requirement.  \\n• Interacted professionally and effectively through verbal and written communication \\nwith global Category Management teams and  IT divisions within the compa ny \\n \\n \\nData Scientist / Senior Analyst                                                                                      Nov 2013 -Sep 2017  \\n \\nClient : Bank of America , Charlotte, NC  \\nEmployer : Info sys, Chennai  \\n \\nTools and Technology: Python,  Data Science,  Machine Learning , Deep Learning,  Natural Language \\nProcessing,  Computer Vision, PySpark , GCP , JIRA, SQL Server , Spyder , Hive , MicroStrategy , Tableau  \\n \\nRoles and Responsibilities:  \\nProject 1  : Confidential   \\n• Building various regression and classification algorithms by using algorithms such as \\nLogistic Regression, Decision Trees, XGboost, Adaboost,KNN and Naïve Bayes’  for \\npredictive analysis and credit market.  • Implemented  K-Means, DB -Scan and Hierarchical Clust ering models for customer \\nsegmentation based on common behavior , profitability , income demography  and usage \\nof banking services  \\n• Worked in collaboration with Product Managers to understand the challenges towards a \\nbanking development and provide solutions with ML & AI techniques  \\n• Developed advanced statistical modelling techniques using various visualizations,  \\nhypothesis tests, chi square test in Python  \\n• Worked on NLP in Information extraction, dependency parsing  and Rule based \\nmatching  for wholesale banking  data  \\n• Implemented  feature engineering techniques which includes missing data imputation, \\ncategorical encoding, discretization, engineering outliers, feature scaling, feature \\npipeline, feature selection for data preprocessing.  \\n• Performed hyperparameter tuning  techniques s uch as GridSearchCV, \\nRandomized SearchCV  and Bayesian  Optimization for better model performance  for \\ndigital transactions  \\n• Face verification  for customer login and managing customer data  to devise new strategy \\nfor better revenue generation  and da ta driven decision s using neural networks in tensor \\nflow  \\n• Worked extensively on Computer Vision using OpenCV on Image manipulation and \\nsegmentation techniques  for credit card and fraud detection by providing security \\nthrough prevention systems  \\n• Implemented full life cycle development of ML projects from data analysis to \\ndeployment as per best practice guidelines.  \\n• Evaluating the predictive models using model performance measures and testing the \\nperformance on test data  \\n• Experience in creating data science pipe lines encompassing Data standardization, \\nFeature extraction, mo del validation and optimization  \\n \\nProject 2: Confidential   \\n• Worked with Data Engineering and ETL team to apply transformations on raw data and \\nextracting the columns needed for modelling.  \\n• Performed feature selection and worked on feature engineering techniques like feature \\nscaling, missing data imputation and outliers imputation  \\n• Implemented Recommendation systems  using collaborative filtering  to analyze \\ncustomer activities based on previo us transactions and recommends loan, deposits and \\noffers that are suitable for customers  retention  \\n• Designed CNN network with pretrained models to classify facial recognition for \\nautomatic login and secure transactions  and optimizing the model for better \\nperformance and speed  \\n• Deployed the model in cloud platform and provided support for model monitoring and \\nre-building.  \\n• Customer data management and using it effectively for business profitability  • Created interactive and visually interactive tableau dashboards which provides easy to \\nuse access to data and to optimize decision making  \\n• Designed various graphs that enables managers and business users to view data at a \\nglance through a range of insightful data visualization  \\n• Data validation after the dashboard produced the data and the earlier user reports to \\nshow the dashboards are reporting accurately by creating test cases  \\n• Interacted professionally and effectively through verbal and written communication \\nwith global Category M anagement teams and IT divisions within the company  \\n• Supported QA/UAT team in testing phase  \\n \\nData Scientist / BI Analyst                                                                                            May 2011   - Nov 2013   \\n \\nClient : United Health Group (UHG), OPTUM Rx , Minneapolis, MN  \\nEmployer : Cognizant , Chennai  \\n \\nTools and Technology: Python,  Machine Learning,  Deep Learning  SQL Server,  Micro Strategy , \\nTableau  \\nProject 1:  Confidential   \\n \\nRoles and Responsibilities:  \\n \\n• Data preprocessing for huge data source selecting important features and applying \\nfeature engineering techniques  and EDA on data  visualization using using Matplotlib, \\nPlotly, searborn  \\n• Predictive modelling for patient segmentation data using statistical  visualizations like \\nboxplot, scatter plot and pair plot and deriving the correlation between features  \\n• Used PCA for feature extrac tion due to huge volume of features in dataset  \\n• Built  machine learning models using algorithms like Logistic Regression  and SVM  for \\nimmune system classification for patients and p rediction of drugs with right course of \\naction  to track and monitor patient health  \\n• Image diagnosis using Deep learning CNN models for image classification  to detect the \\nposition of inflammation in medical images stored as dicom files which contains the raw \\nimage arrays for pixel data  \\n• Built Object detection model with CNN using tra nsfer learning to fine tune the model \\nand defining optimal optimizers, loss functions, epochs, learning rate, batch size, check \\npointing  and early stopping.  \\n• Used performance evaluation metrics for the model on how changing different hyper \\nparameters  could lead to model efficiency  • Used hypothesis testing and visualization using python using customer data and used \\nthe statistical evidence to make decision  \\n• Used Confusion matrix , to evaluate the  accuracy of  results  and identify the errors  \\n• Communicate the resul ts to the heal thcare stakeholders.  \\n \\n \\nProject 2:  Confidential   \\n• Involved in creating MicroStrategy Schema objects - Attributes, Fact and Hierarchies  \\n• Worked extensively on developing Complex Reports  \\n• Created Simple Documents and Dashboards with datasets  \\n• Developed dashboards using selectors, Panel stacks and Auto texts  \\n• Created Widgets for better visualization.  \\n• Created Simple, Compound, Derived and Conditional Metrics  \\n• Created Report Filters - Attribute qualification, Metric Qualifications, Report as Filter \\nand Prompts - Dynamic Date prompts, Value, Hierarchy and Object prompts  \\n• Worked in Formatting of Reports by creating customized Auto Styles, applying banding, \\nthresholds, sorting and subtotals  \\n• Used Object Manager for report migration  \\n• Tested all the reports by running queries against the warehouse. Also compared those \\nqueries with the  MicroStrategy SQL  engine generated queries  \\n \\nCertifications   \\n \\n\\uf0b7 MicroStrategy 10 Certified Designer (MCD), 2017  \\n\\uf0b7 MicroStrategy 10 Certified Platform Administrator (MCPA), 2017  \\n\\uf0b7 MicroStrategy 10 Certified Analyst (MCA), 2017  \\n\\uf0b7 Advanced Analytics Reporting by MicroStrategy, 2017  \\n\\uf0b7 MicroStrategy Certified Report Developer, 2014  \\n\\uf0b7 Tableau Author (Badge), 2020  \\n\\uf0b7 Tableau Consumer (Badge), 2020  \\n\\uf0b7 Tableau Analyst (Badge), 2020  \\n\\uf0b7 Tableau Data Scientist (Badge), 2020  \\n\\uf0b7 Tableau Designer (Badge), 2020  \\n \\n \\n \\n Achievements rtifications  \\n \\n  \\n\\uf0b7 Continuous Excellent grade in all courses of PGP -AIML  with star performer award  \\n\\uf0b7 “Excellence Award” for project recognition with Infosys  \\n\\uf0b7 “Star of the Month” award for academic excellence in Cognizant  \\n\\uf0b7 Completed IELTS with  band score of 7.5 during Feb 2019.  \\n\\uf0b7 Department topper (SSN Eng. College) in Anna University Examination, Nov 2010  \\n\\uf0b7 Got a cash award of Rs 5000/ - for scoring above 1150 out of 1200(scored 1154) in Higher \\nSecondary Examinations with centum in Mathematics  \\n\\uf0b7 Scho ol topper in  High School and district topper  (1037/1100) with centum in Mathematics \\n(200/200)  \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4350d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to split the text using Character Text Split such that it sshould not increse token size\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13fe3e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9e38c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kpuni\\Anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Download embeddings from OpenAI\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "196cf528",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_search = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5b6c53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x21543aa0a60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6342c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07efe736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kpuni\\Anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9221b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kpuni\\Anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The project aims to implement various machine learning and artificial intelligence techniques to solve problems related to banking, image classification, natural language processing, and object detection. The project also involves developing recommendation systems and building models for data analysis and decision making in the retail banking sector.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What the project aims at\"\n",
    "docs = document_search.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1cee1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' MicroStrategy 10 Certified Designer (MCD), 2017\\nMicroStrategy 10 Certified Platform Administrator (MCPA), 2017\\nMicroStrategy 10 Certified Analyst (MCA), 2017\\nAdvanced Analytics Reporting by MicroStrategy, 2017\\nMicroStrategy Certified Report Developer, 2014\\nTableau Author (Badge), 2020\\nTableau Consumer (Badge), 2020\\nTableau Analyst (Badge), 2020\\nTableau Data Scientist (Badge), 2020\\nTableau Designer (Badge), 2020'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"list the certifications\"\n",
    "docs = document_search.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf39ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
